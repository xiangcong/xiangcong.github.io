---
layout: post
title: 深度残差网络
description: resnet
categories: 深度学习
tags: [dnn]
---  

# ResNet 残差网络

### 在ResNet之前深度网络不是越深越好
当网络层数增加时，会发生**梯度弥散**的问题，这个问题近年来，可以利用normalized initialization以及batch normalization来改善，似的十几层的网络（如googlenet vgg）可以收敛。

但是随着层数进一步增大，分类的精度会达到饱和，继续加深层数后精度甚至会降低，并且这个不是由over fit造成的（因为训练误差也会增大）

## 思考如何收敛
原来是让每一层学习以逼近一个最优映射x -> H(x)

现在让每一层的输出逼近H(x)-x（残差）即 x -> H(x)-x

如下图所示，作者比较了18层和34层，相似结构的resNet和plainNet

![plainVSres](https://github.com/xiangcong/xiangcong.github.io/blob/master/images/compare_plainNet_resNet.png?raw=true)

从图中可以看出，plainNet 34层网络的训练和验证误差都比18层的大

而ResNet 更深的网络有着更好的效果，说明作者的resNet结构是很有效的

## 基本结构如下图所示

![building block](https://github.com/xiangcong/xiangcong.github.io/blob/master/images/resnet_building_block.png?raw=true)

y = F(x, {Wi}) + Wsx

线性映射Ws当且仅当x与F维度不一致时存在，加号表示对应元素相加，
全接连网络和卷积网络都可以

**维度不一致怎么搞**

* zero padding & stride 2
* projection -> conv


## PS

### ResNet的参数个数没有VGG多

### ResNet的结构

![ResNet结构](https://github.com/xiangcong/xiangcong.github.io/blob/master/images/resNet_Architecture.png?raw=true)



 

