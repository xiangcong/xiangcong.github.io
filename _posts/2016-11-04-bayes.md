---
layout: post
title: 贝叶斯分类器
description: 周志华之机器学习第七章
categories: 机器学习
tags: [机器学习]
---  

# 贝叶斯分类器  

#### 最优贝叶斯分类器  
对于每个样本x，选择能使后验概率P(C|x)最大的类别标记

### 两种模型

1. 判别式模型：直接对P(C|x)建模。（决策树，BP神经网络，SVM等）
2. 生成式模型：先对联合概率P(x,C)建模，再由此获得P(C|x)  


####生成式  

<img src="http://www.forkosh.com/mathtex.cgi?
P(C|x) &= \frac{P(C,x)}{P(x)} =\frac{P(x|C)P(C)}{P(x)}  \propto P(C)P(x|C)
">  

其中：

P(x)称为归一化“证据”因子。  

可以看出，生成式模型中的P(C|x)正比于**先验**乘以**似然**  

先验P(C)可以从训练数据中统计得出。  

P(x|C)需要进一步估计。常用策略是，假定其具有某种确定的概率粉笔，再基于训练样本对参数进行估计。即假定P(x|C)有确定的形式并被参数
<img src="http://www.forkosh.com/mathtex.cgi?
\theta_c
"> 
 唯一确认。将P(x|C)记为
 <img src="http://www.forkosh.com/mathtex.cgi?
P(x|\theta_c)
"> 
并利用训练集D估计
<img src="http://www.forkosh.com/mathtex.cgi?
\theta_c
"> 

## 参数估计
分为两个学派：  

1. 频率派：参数虽未知，但是客观存在的固定值
2. 贝叶斯派：参数是未观察到的随机变量，本身也有分布。可以假定其服从一个先验分布，再基于观察到的数据来计算参数的后验分布。

本blog讨论频率派的**极大似然估计**(MLE)。  

<img src="http://www.forkosh.com/mathtex.cgi?
P(D_c|\theta_c) = \prod_{x\in D_c}P(x|\theta_c)
"> 

概率连乘将造成下溢，故改为使用对数似然。

<img src="http://www.forkosh.com/mathtex.cgi?
LL(\theta_c) = log(P(D_c|\theta_c)=\sum_{x\in D_c}logP(x|\theta_c)
"> 

极大似然估计估计出的参数为： 

<img src="http://www.forkosh.com/mathtex.cgi?
\hat\theta_c = \arg\max_{\theta_c} LL(\theta_c)
"> 

example:  

P(x|c) ~ N(u_c, sigma_c^2)

则根据极大似然估计，可以得到

<img src="http://www.forkosh.com/mathtex.cgi?
\hat\mu_c = \frac{1}{|D_c|}\sum_{x\in D_c}x
">   

<img src="http://www.forkosh.com/mathtex.cgi?
\hat\sigma_c^2 = \frac{1}{|D_c|}\sum_{x\in D_c}(x-\hat\mu_c)(x-\hat\mu_c)^T
"> 

结果的好坏依赖于假设的概率分布是否符合潜在的真是数据分布。 往往需要再一定程度上利用关于应用任务本身的经验知识。  

## 朴素贝叶斯分类器  

由于P(x|c)是所有属性上的联合概率，难以从有限的训练样本中直接估计而得。例如d个属性都是二值的，样本空间可能有2^d个取值，这个值往往远大于训练样本数m，很多样本在训练集中根本没有出现。

“朴素”贝叶斯->属性条件独立性假设  

<img src="http://www.forkosh.com/mathtex.cgi?
P(c|x) = \frac{P(c)P(x|c)}{P(x)}=\frac{P(c)}{P(x)}\prod_iP(x_i|c)
"> 

<img src="http://www.forkosh.com/mathtex.cgi?
h(x)=\arg\max_{c\in \mathbf{y}}P(c)\prod_{i=1}^dP(x_i|c)
"> 

其中 

<img src="http://www.forkosh.com/mathtex.cgi?
P(c)=\frac{|D_c|}{|D|}
"> 

对于离散属性：  

<img src="http://www.forkosh.com/mathtex.cgi?
P(x_i|c) = \frac{|D_{c,x_i}|}{|D_c|}
">   

D_{c,x_i}表示D_c中在属性i上取值为x_i的样本组成的集合 

对于连续属性： 

可以假定P(x_i|c)服从高斯分布 

<img src="http://www.forkosh.com/mathtex.cgi?
P(x_i|c) = \frac{1}{\sqrt(2\pi)\sigma_{c,i}}exp(-\frac{(x_i-\mu_{c,i})^2}{2\sigma_{c,i}^2})
">   


### 平滑处理 

为避免其他属性携带的信息被训练集中未出现的属性值“抹去”，通常进行“平滑“处理，常用”拉普拉斯修正“  

<img src="http://www.forkosh.com/mathtex.cgi?
\hat P(c) = \frac{|D_c|+1}{|D|+N}
">   

<img src="http://www.forkosh.com/mathtex.cgi?
\hat P(x_i|c) = \frac{|D_{c,x_i}|+1}{|D_c|+N_i}
">  

N表示训练集D中可能的类别数，Ni表示第i属性可能的取值数  

## 半朴素贝叶斯分类器 

对以下两点进行tradeoff:  

1. 联合概率难求
2. 属性间依赖不可忽略  

ODE -> One Dependent Estimator 独依赖估计  

详见 机器学习，周志华 P156  

## 贝叶斯网  

用有向无环图来刻画属性间的依赖关系，用条件概率来表述属性的联合概率分布。  

B = \<G, \Theta>  

<img src="http://www.forkosh.com/mathtex.cgi?
P_B(x_1,x_2,\cdots,x_d) = \prod_{i=1}^dP_B(x_i|\pi_i) = \prod_{i=1}^d \Theta_{x_i|\pi_i}
">  

**首要任务**：根据训练数据集来找出结构最“恰当”的贝叶斯网络->评分搜索。  

<img src="http://www.forkosh.com/mathtex.cgi?
S(B|D)=f(\theta)|B|-LL(B|D)
">  

## EM算法  

实际问题中可能存在隐变量的情况，令X表示已观测变量集，Z表示隐变量集，\Theta表示模型参数。若欲对\Theta做极大似然估计，则应最大化对数似然：  

<img src="http://www.forkosh.com/mathtex.cgi?
LL(\Theta|X,Z)=lnP(X,Z|\Theta)
"> 

然而由于Z是隐变量，无法直接求解。此时我们可通过对Z计算期望，来最大化已观测数据的对数“边际似然”。

<img src="http://www.forkosh.com/mathtex.cgi?
LL(\Theta|X)=lnP(X|\Theta)=ln\sum_ZP(X,Z|\Theta)
"> 

EM(Expectation-Maximization)算法是常用的估计参数隐变量的利器，它是一种迭代的方法，基本思想是：做参数Theta已知，则可根据训练数据推断出最优隐变量Z的值(E步)；反之，若Z的值已知，则可方便的对参数Theta做极大似然估计（M步）。

重复迭代以下两部直至收敛：

1. 基于Theta^t推算隐变量Z的期望，记为Z^t
2. 基于已观测的变量X和Z^t对参数Theta做极大似然估计，记为Theta^(t+1)

若不是计算Z的期望，则改为计算Z的概率分布P(Z|X,Theta^t)

1.
<img src="http://www.forkosh.com/mathtex.cgi?
Q(\Theta|\Theta^t)=E_{Z|X,\Theta^t}LL(\Theta|X,Z)
">   
2.
<img src="http://www.forkosh.com/mathtex.cgi?
\Theta^{t+1}=\arg\max_\Theta Q(\Theta|\Theta^t)
"> 

可用贾森不等式来证明算法的有效性，具体参见[blog](http://blog.csdn.net/zouxy09/article/details/8537620)

 