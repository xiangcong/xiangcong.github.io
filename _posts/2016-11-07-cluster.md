---
layout: post
title: 聚类
description: 周志华之机器学习第九章
categories: 机器学习
tags: [机器学习]
---  

# 聚类

将数据集中的样本分为若干个通常是不想交的子集，每个自己成为一个“簇”。  

聚类可用于寻找数据的内在分布，也可作为分类等其他学习任务的前驱过程。  

## 性能度量（有效性指标）  

一般而言，聚类希望：  

1. 簇内相似度高
2. 簇间相似度低  

性能度量大致分为两类：  

1. 外部指标：讲聚类结果与某个“参考”模型进行比较
2. 内部指标 直接考察聚类结果而不利用任务参考模型  

具体各个指数P198  

## 距离计算  

距离函数dist(.,.)一般需要满足某些性质:  

1. 非负性：dist(xi, xj) >= 0
2. 同一性：dist(xi, xj) = 0 当且仅当 xi = xj
3. 对称性：dist(xi, xj) = dist(xj, xi)
4. 传递性：dist(xi, xj) <= dist(xi, xk) + dist(xk, xj)（不一定满足，e.g. 人，人马，马）

属性可划分为:  

1. 连续属性
2. 离散属性

然而讨论距离时，更关心的是“序”， 即分为：  

1. 有序属性
2. 无序属性 （例如{飞机，火车，轮船}这样的离散属性）（也可以考虑将其转换为bummy code）  

对于有序属性，最常用的是Minkowski distance（闵可夫斯基距离）：
<img src="http://www.forkosh.com/mathtex.cgi?
dist_{mk}(x_i, x_j) = \left(\sum_{u=1}^{n}|x_{iu}-x_{ju}|^p\right)^{\frac{1}{p}}
"> 

上式即为xi和xj的Lp范数
<img src="http://www.forkosh.com/mathtex.cgi?
||x_i-x_j||_p
"> 

对于无序属性，可采用VDM(Value Difference Metric)

<img src="http://www.forkosh.com/mathtex.cgi?
VDM_p(a,b)=\sum_{i=1}^k\left|\frac{m_{u,a,i}}{m_{u,a}}-\frac{m_{u,b,i}}{m_{u,b}}\right|^p  
"> 

m_u,a表示在属性u上取值为a的样本数，m_u,a,i表示在第i个样本簇中在属性u上取值为a的样本数，k为样本簇数。

**以上均为直接定义距离，还可以通过距离度量学习来实现**  

## 原型聚类  
又称为基于原型的聚类，此类算法假设聚类结构能通过一组原型刻画，在实现聚类任务中极为常用。  

### k均值算法

<img src="http://www.forkosh.com/mathtex.cgi?
\min E= \sum_{i=1}^k\sum_{x\in C_i}||x-\mu_i||^2_2
"> 

基于贪心算法，迭代更新直至均值向量不再更新

### 学习向量量化 Learning Vector Quantization LVQ 

需要数据样本带有类别标记，定义q个原型向量和学习率。迭代更新原型向量。  

### 高斯混合聚类 

用EM算法实现  

## 密度聚类 
又称为基于密度的聚类，此类算法假设聚类结构能够通过样本分布的紧密程度确定。  

DBSCAN算法。。。 

## 层次聚类 
试图在不同层次对数据集进行划分，从而形成树形的聚类结构。  
AGNES->不断的合并簇
