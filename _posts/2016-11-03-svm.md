---
layout: post
title: 支持向量机
description: 周志华之机器学习第六章
categories: 机器学习
tags: [机器学习]
---  

# 支持向量机 SVM

要解决的问题：对于二分类问题，找一个最优分界面。何为最优？margin最大->可以抵抗的数据干扰最大。

![svm](https://github.com/xiangcong/xiangcong.github.io/blob/master/images/svm.png?raw=true)  

<img src="http://www.forkosh.com/mathtex.cgi?
\arg\min_w \frac{1}{2}||w||_2^2  
">   

s.t. 

<img src="http://www.forkosh.com/mathtex.cgi?
y_i(w^T x_i + b) \geq 1
"> 

约束最优化问题根据拉格朗日乘子法可以变化为：  

<img src="http://www.forkosh.com/mathtex.cgi?
L(w,b) = \frac{1}{2}||w||^2_2 + \sum_{i=1}^{m}\alpha_i(1-y_i(w^Tx_i+b))  
">   

L(w,b)分别关于w,b求导并令为0，可得：  

<img src="http://www.forkosh.com/mathtex.cgi?
\sum_{i=1}^{m}\alpha_iy_i = 0  
"> 


<img src="http://www.forkosh.com/mathtex.cgi?
w = \sum_{i=1}^{m}\alpha_iy_ix_i  
">   

根据KKT条件，有： 

<img src="http://www.forkosh.com/mathtex.cgi?
\alpha_i(y_i(w^Tx_i+b)-1) = 0
">      
<img src="http://www.forkosh.com/mathtex.cgi?
y_i(w^Tx_i+b)-1 \geq 0
">  
<img src="http://www.forkosh.com/mathtex.cgi?
\alpha_i \geq 0
"> 

如果 ai = 0, 那么该点是支持向量
如果 ai > 0, 那么该点不是支持向量（不会对w有贡献）

将w带入L(w,b)，可将其转化为对偶问题  

<img src="http://www.forkosh.com/mathtex.cgi?
\max W(\alpha) = \sum_{i=1}^m \alpha_i - \frac{1}{2}\sum_{i=1}^m\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_jx_i^Tx_j   
">  

s.t.  

<img src="http://www.forkosh.com/mathtex.cgi?
\alpha_i \geq 0
">    

<img src="http://www.forkosh.com/mathtex.cgi?
\sum_{i=1}^m\alpha_iy_i = 0
">  

这是一个二次规划问题，可使用通用的二次规划算法来求解，然而该问题的规模正比于训练样本的数量，这会在实际任务中造成很大的开销。

可用SMO高效求解：  

1. 选取一堆更新变量 ai, aj （选取的时候也有讲究，P125）
2. 固定ai, aj以外的参数，求解并更新 ai, aj  

当确定了各a值之后，  

<img src="http://www.forkosh.com/mathtex.cgi?
w = \sum_{i=1}^{m}\alpha_iy_ix_i  
">  

考虑任意支持向量(xs, ys)：  

<img src="http://www.forkosh.com/mathtex.cgi?
y_s(\sum_{i\in S}\alpha_iy_ix_i^Tx_s + b) = 1
">  

其中S为所有支持向量组成的下标集，可选取任意支持向量求解b  

实际采用更鲁棒的做法：  

<img src="http://www.forkosh.com/mathtex.cgi?
b = \frac{1}{|S|}\sum_{s\in S}(y_s - \sum_{i\in S}\alpha_iy_ix_i^Tx_s)  
">  

## 核函数  

为了解决线性不可分问题，希望将输入空间映射到高维空间，即  

<img src="http://www.forkosh.com/mathtex.cgi?
x \rightarrow \phi(x)  
">  

但是直接求解
<img src="http://www.forkosh.com/mathtex.cgi?
\phi(x)^T\phi(y)  
">  很困难   

希望能一个函数k，使得  

<img src="http://www.forkosh.com/mathtex.cgi?
\mathcal{K}(x_i, x_j) = \phi(x_i)^T\phi(x_j)   
">   

即xi与xj在映射后的特征空间中的内积等于它们在原始空间中通过函数k计算的结果  

通过类似的求解方式，可得：  

<img src="http://www.forkosh.com/mathtex.cgi?
f(x)&=w^T\phi^T(x)+b\\
&=\sum_i\alpha_iy_i\phi(x-i)^T\phi(x)+b\\
&=\sum_i\phi_iy_i\mathcal{K}(x,x_i)+b
"> 

### 核函数需要满足什么条件

1. 对称函数
2. 对任意数据 D={x1, x2, ...., xm}  
核矩阵 K = [...]半正定  

常用核函数： 

1. 线性核
2. 多项式核
3. 高斯核
4. 拉普拉斯核
5. sigmoid核  

## 软间隔与正则化

太严格的线性可分可能会过拟合，导致泛化性能不好。因而考虑软间隔，理想形式如下：  

<img src="http://www.forkosh.com/mathtex.cgi?
\min_{w,b}\frac{1}{2}||w||^2+C\sum_il_{0/1}(y_i(w^Tx_i+b)-1)  
"> 

其中  

<img src="http://www.forkosh.com/mathtex.cgi?
l_{0/1}(z)=  
\begin{cases}
1 & z\le 0 \\
0 & otherwise
\end{cases}
"> 

当C趋向于无穷大时，该模型退化为hard margin

然而,该函数非凸，不连续，需要寻找替代函数：  

#### hinge loss
<img src="http://www.forkosh.com/mathtex.cgi?
l(z) = max(0, 1-z)
"> 
#### exponential loss
<img src="http://www.forkosh.com/mathtex.cgi?
l(z)=exp(-z)
"> 
#### logistic loss
<img src="http://www.forkosh.com/mathtex.cgi?
l(z)=log(1+exp(-z))
"> 

![loss funciton](https://github.com/xiangcong/xiangcong.github.io/blob/master/images/loss%20function.jpg?raw=true)

图引用自 [机器学习，周志华](http://cs.nju.edu.cn/zhouzh/)

## 松弛变量 

本质上是hinge loss

<img src="http://www.forkosh.com/mathtex.cgi?
\min_{w,b,\xi_i} \frac{1}{2}||w||^2 + C\sum_i\xi_i
">   

s.t.  

<img src="http://www.forkosh.com/mathtex.cgi?
y_i(w^Tx_i+b)\geq 1-\xi_i \\
\xi_i \geq 0\\
i = 1, 2, \cdots, m
"> 

同样转化为对偶问题，并求偏导令为0，得:  


<img src="http://www.forkosh.com/mathtex.cgi?
\max W(\alpha) = \sum_{i=1}^m \alpha_i - \frac{1}{2}\sum_{i=1}^m\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_jx_i^Tx_j   
"> 

s.t.  

<img src="http://www.forkosh.com/mathtex.cgi?
0 \leq \alpha_i \leq C
">    

<img src="http://www.forkosh.com/mathtex.cgi?
\sum_{i=1}^m\alpha_iy_i = 0
">  

当ai=0时，样本点落在正确的区域；  
当ai=C时，样本点落在间隔内（分错);  
其余情况下，样本点是支持向量；

### 如果用对率损失函数来替代0/1损失 

会得到对率回归模型。实际上，SVM与logistic regression的优化目标非常相近，通常情况下它们的性能也相当。

与对率回归模型相比：

1. SVM的输出没有自然的概率意义，欲得到概率输出需进行特殊处理。
2. 对率回归能直接用于多分类问题，SVM则需要进行推广。

### 结构风险与经验风险  

<img src="http://www.forkosh.com/mathtex.cgi?
\min_f \Omiga(f) + C\sum_{i=1}^ml(f(x_i), y_i))
"> 

优化目标中的第一项称为结构风险，用于描述模型f的某些性质（划分超平面的“间隔”大小）；第二项称为经验风险，用于描述模型与训练数据的契合程度（训练集上的误差）

从经验风险最小化的角度来看，结构风险标书了我们希望获得具有何种性质的模型（例如希望获得复杂度较小的模型），这为引入领域知识和用户意图提供了途径；另一方面，该信息有助于削减假设空间，从而降低了最小化训练误差的过拟合风险。从这个角度，该问题可称为正则化问题，结构风险称为正则化项，C则称为正则化常数。Lp范数是常用的正则化项，L2范数倾向于w的分量取值尽量均衡，即非0分量个数尽量稠密。L0和L1范数倾向w的分量尽量稀疏，即非零个数尽量少。

## 支持向量回归 SVR

<img src="http://www.forkosh.com/mathtex.cgi?
\min_{w,b}\frac{1}{2}||w||^2+C\sum_{i=1}^ml_{\epsilon}(f(x_i)-y_i)
">  

其中le是e-不敏感损失函数：

<img src="http://www.forkosh.com/mathtex.cgi?
l_{\epsilon}(z)=
\begin{cases}
0 & |z|\leq\epsilon \\
|z|-\epsilon & otherwise
\end{cases}
"> 

根据软间隔可得到类似推导



